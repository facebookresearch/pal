{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model: transformer architecture\n",
    "\n",
    "We will try to learn modular addition and substraction with a transformer.\n",
    "Many codebase that ensure efficient implementation can be found online, e.g., with [`NanoGPT`](https://github.com/karpathy/nanoGPT), or [`xFormer`](https://github.com/facebookresearch/xformers).\n",
    "\n",
    "I recommend watching Andrej Karpathy's [lectures](https://www.youtube.com/playlist?list=PLAqhIrjkxbuWI23v9cThsA9GvCAUhRvKZ) to ground yourself in the basics of deep learning and transformers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us start by loading the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_4134001/3566878705.py:12: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  x_train = torch.load(save_dir / 'x_train.pt')\n",
      "/tmp/ipykernel_4134001/3566878705.py:13: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  y_train = torch.load(save_dir / 'y_train.pt')\n",
      "/tmp/ipykernel_4134001/3566878705.py:14: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  x_test = torch.load(save_dir / 'x_test.pt')\n",
      "/tmp/ipykernel_4134001/3566878705.py:15: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  y_test = torch.load(save_dir / 'y_test.pt')\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from llmtuto.config import DATA_DIR\n",
    "\n",
    "problem = 'multi_base'\n",
    "save_dir = DATA_DIR / problem\n",
    "\n",
    "x_train = torch.load(save_dir / 'x_train.pt')\n",
    "y_train = torch.load(save_dir / 'y_train.pt')\n",
    "x_test = torch.load(save_dir / 'x_test.pt')\n",
    "y_test = torch.load(save_dir / 'y_test.pt')\n",
    "\n",
    "# # Check for data correctness\n",
    "# assert (x_train.sum(axis=1) % 60 == y_train).all()\n",
    "# assert (x_test.sum(axis=1) % 60 == y_test).all()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Token embeddings\n",
    "\n",
    "Rather than discrete tokens, transformer works with sentences in $\\mathbb{R}^d$.\n",
    "Therefore, tokens are embed $\\mathbb{R}^d$ through a look-up table (`one_hot(x_train) @ token_emb`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "vocab_size = torch.max(x_train) + 1\n",
    "emb_dim = 8\n",
    "\n",
    "token_emb = nn.Embedding(vocab_size, emb_dim) # word token embedding\n",
    "\n",
    "# Comments abbreviations:\n",
    "# N: batch size\n",
    "# L: sequence length\n",
    "# V: vocabulary size\n",
    "# E: embedding dimension\n",
    "\n",
    "x_emb = token_emb(x_train)  # one_hot(x_train) @ wte: (N, L, V, E) @ (V, E) -> (N, L, E)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([28773, 4, 8])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_emb.shape"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Weight tying\n",
    "\n",
    "We will predict a token `y` through a score (which will be interpreted as a logit by the cross-entropy loss):\n",
    "\n",
    "```s(x, y) = g(token_emb(x)) @ token_emb(y).T  = g(x) @ token_emb.T @ one_hot(y)```.\n",
    "\n",
    "This corresponds to adding a linear layer at the end of the network and tying weights between the embedding and the output \"un-embedding\" layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "unemb = nn.Linear(emb_dim, vocab_size, bias=False)    # un-embedding layer (E, V)\n",
    "unemb.weight = token_emb.weight                       # tie weights"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Attention block\n",
    "\n",
    "We start with a simple implementation of a single attention head.\n",
    "You can check Andrej Karpathy's videos for a more detailed explanation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "q_mat = nn.Linear(emb_dim, emb_dim, bias=False)       # query matrix (E, E)\n",
    "k_mat = nn.Linear(emb_dim, emb_dim, bias=False)       # key matrix\n",
    "v_mat = nn.Linear(emb_dim, emb_dim, bias=False)       # value matrix\n",
    "\n",
    "q = q_mat(x_emb)                                      # query (N, L, E) @ (E, E) -> (N, L, E)\n",
    "k = k_mat(x_emb)                                      # key \n",
    "v = v_mat(x_emb)                                      # value\n",
    "\n",
    "attn = q @ k.transpose(-1, -2) / math.sqrt(emb_dim)   # attention (N, L, E) @ (N, E, L) -> (N, L, L)\n",
    "# When attention is causal, we should not attend to previous tokens\n",
    "causal = True\n",
    "if causal:\n",
    "    L = x_emb.shape[1]\n",
    "    mask = torch.tril(torch.ones(L, L)) == 0\n",
    "    attn = attn.masked_fill(mask, float('-inf'))\n",
    "attn = torch.softmax(attn, dim=-1)                    # softmax over last dimension\n",
    "\n",
    "z = attn @ v                                          # (N, L, L) @ (N, L, E) -> (N, L, E)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us now implement several heads, to do so, we cut the different matrices per heads (so to \"fuse\" matrix multiplication)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comments abbreviations:\n",
    "# H: number of heads\n",
    "\n",
    "N, L, E = x_emb.size()\n",
    "H = 2\n",
    "\n",
    "q_heads = q.view(N, L, H, E // H).transpose(1, 2)               # (N, L, E) -> (N, L, H, E / H) -> (N, H, L, E / H)\n",
    "k_heads = k.view(N, L, H, E // H).transpose(1, 2)\n",
    "v_heads = v.view(N, L, H, E // H).transpose(1, 2)\n",
    "\n",
    "attn = q_heads @ k_heads.transpose(-1, -2) / math.sqrt(E // H)  # (N, H, L, E / H) @ (N, H, E / H, L) -> (N, H, L, L)\n",
    "if causal:\n",
    "    mask = torch.tril(torch.ones(L, L)).view(1, 1, L, L) == 0\n",
    "    attn = attn.masked_fill(mask, float('-inf'))\n",
    "attn = F.softmax(attn, dim=-1)\n",
    "z = attn @ v_heads                                             # (N, H, L, L) @ (N, H, L, E / H) -> (N, H ,L, E / H)\n",
    "z = z.transpose(1, 2).contiguous().view(N, L, E)               # (N, H, L, E / H) -> (N, L, H, E / H) -> (N, L, E)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now turn to a faster implementation based on the \"fusion\" of many operation at detailed in the [flash attention paper](https://arxiv.org/abs/2205.14135) and implemented by pytorch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing correctness: tensor(2.3842e-07, grad_fn=<MaxBackward1>)\n"
     ]
    }
   ],
   "source": [
    "attn_mat = nn.Linear(emb_dim, 3 * emb_dim, bias=False)          # attention matrix (E, E)\n",
    "\n",
    "N, L, E = x_emb.size()\n",
    "training = True\n",
    "causal = False\n",
    "dropout = 0\n",
    "n_head = 2\n",
    "\n",
    "# calculate query, key, values for all heads in batch and move head forward to be the batch dim\n",
    "q, k, v  = attn_mat(x_emb).split(emb_dim, dim=2)\n",
    "q = q.view(N, L, H, E // H).transpose(1, 2)\n",
    "k = k.view(N, L, H, E // H).transpose(1, 2)\n",
    "v = v.view(N, L, H, E // H).transpose(1, 2)\n",
    "\n",
    "# efficient attention using Flash Attention CUDA kernels\n",
    "z = F.scaled_dot_product_attention(q, k, v, attn_mask=None, dropout_p=dropout if training else 0, is_causal=causal)\n",
    "z1 = z.clone()\n",
    "z = z.transpose(1, 2).contiguous().view(N, L, E)\n",
    "z2 = z.clone()\n",
    "\n",
    "# Check computation\n",
    "attn = q @ k.transpose(-1, -2) / math.sqrt(E // H)\n",
    "if causal:\n",
    "    L = x_emb.shape[1]\n",
    "    mask = torch.tril(torch.ones(L, L)) == 0\n",
    "    attn = attn.masked_fill(mask, float('-inf'))\n",
    "attn = torch.softmax(attn, dim=-1)\n",
    "z_bis = attn @ v\n",
    "z_bis = z_bis.transpose(1, 2).contiguous().view(N, L, E)\n",
    "print('Testing correctness:', (z_bis - z).abs().max())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are now ready to write a self-attention module.\n",
    "For readibility, we will pass all arguments into a `config` object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing correctness: tensor(3.5763e-07)\n"
     ]
    }
   ],
   "source": [
    "from llmtuto.model.transformer import SelfAttention, TransformerConfig\n",
    "\n",
    "\n",
    "config = TransformerConfig(\n",
    "    vocab_size = vocab_size, \n",
    "    emb_dim = emb_dim, \n",
    "    n_head = n_head, \n",
    "    attn_dropout = dropout,\n",
    "    causal = causal\n",
    ")\n",
    "\n",
    "state_dict = {\n",
    "    'query.weight': attn_mat.weight[:emb_dim],\n",
    "    'key.weight': attn_mat.weight[emb_dim:2*emb_dim],\n",
    "    'value.weight': attn_mat.weight[2*emb_dim:],\n",
    "    'output.weight': torch.eye(emb_dim, emb_dim),\n",
    "}\n",
    "\n",
    "self_att = SelfAttention(config)\n",
    "self_att.load_state_dict(state_dict)\n",
    "\n",
    "with torch.no_grad():\n",
    "    z_tres = self_att(x_emb)\n",
    "    print('Testing correctness:', (z_tres - z).abs().max())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similarly we could write a cross attention module. \n",
    "Let us consider the test data to create a new sequence to attend to."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing correctness: tensor(2.3842e-07)\n"
     ]
    }
   ],
   "source": [
    "from llmtuto.model.transformer import CrossAttention\n",
    "\n",
    "x2_emb = token_emb(x_test[:, :3])\n",
    "x2_emb.shape, x_emb.shape\n",
    "\n",
    "xattn = nn.Linear(E, E, bias=False)      # query\n",
    "yattn = nn.Linear(E, 2 * E, bias=False)  # key, value\n",
    "\n",
    "N_new = min(x_emb.size(0), x2_emb.size(0))\n",
    "x, y = x_emb[:N_new], x2_emb[:N_new]\n",
    "S = y.size(1)\n",
    "\n",
    "# Query, key, value\n",
    "# (N, L, E) @ (E, E) -> (N, L, E)\n",
    "q  = xattn(x)\n",
    "# (N, S, E) @ (E, 2 * E) -> (N, S, 2 * E) -> (N, S, E) x 2\n",
    "k, v  = yattn(y).split(E, dim=2)\n",
    "# reformating: (N, LS, E) -> (N, LS, H, E / H) -> (N, H, LS, E / H)\n",
    "q = q.view(N_new, L, H, E // H).transpose(1, 2)\n",
    "k = k.view(N_new, S, H, E // H).transpose(1, 2)\n",
    "v = v.view(N_new, S, H, E // H).transpose(1, 2)\n",
    "\n",
    "# Attention with (q, k): (N, H, L, E / H) @ (N, H, E / H, S) -> (N, H, L, S)\n",
    "# Value with v:          (N, H, L, S) @ (N, H, S, E / H) -> (N, H ,L, E / H)\n",
    "z = F.scaled_dot_product_attention(\n",
    "    q, k, v, attn_mask=None, dropout_p=dropout if training else 0, is_causal=causal\n",
    ")\n",
    "# reformating:           (N, H, L, E / H) -> (N, L, H, E / H) -> (N, L, E)\n",
    "z = z.transpose(1, 2).contiguous().view(N_new, L, E)\n",
    "\n",
    "\n",
    "config = TransformerConfig(\n",
    "    vocab_size = vocab_size, \n",
    "    emb_dim = emb_dim, \n",
    "    n_head = n_head, \n",
    "    attn_dropout = dropout,\n",
    "    causal = causal\n",
    ")\n",
    "\n",
    "state_dict = {\n",
    "    'query.weight': xattn.weight,\n",
    "    'key.weight': yattn.weight[:emb_dim],\n",
    "    'value.weight': yattn.weight[emb_dim:],\n",
    "    'output.weight': torch.eye(emb_dim, emb_dim),\n",
    "}\n",
    "cross_att = CrossAttention(config)\n",
    "cross_att.load_state_dict(state_dict)\n",
    "\n",
    "with torch.no_grad():\n",
    "    z_tres = cross_att(x, y)\n",
    "    print('Testing correctness:', (z_tres - z).abs().max())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Positional encodings\n",
    "\n",
    "In LLaMa and Mistral implementations, a [ROPE](https://arxiv.org/pdf/2104.09864.pdf) position encoding is added inside each attention block.\n",
    "It consists in decoupling the difference queries into a different frequencies, so that queries are more likely be attend keys-values of tokens that are close in the sequence.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_rope_freqs(seq_len, head_emb_dim, theta: float = 10_000):\n",
    "    \"\"\"\n",
    "    Returns the frequencies for the positional encoding.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    seq_len: int\n",
    "        The length of the sequence.\n",
    "    head_emb_dim: int\n",
    "        The dimension of the head embedding (E / H).\n",
    "    theta: float\n",
    "        An angle parameter.\n",
    "    \"\"\"\n",
    "    freqs = 1.0 / (theta ** (torch.arange(0, head_emb_dim - 1, 2).float() / head_emb_dim))\n",
    "    t = torch.arange(seq_len, device=freqs.device).float()\n",
    "    out = (t.unsqueeze(-1) * freqs.unsqueeze(0))\n",
    "    out = torch.polar(torch.ones_like(out), out)\n",
    "    return out\n",
    "\n",
    "\n",
    "def rope(qk, angles):\n",
    "    \"\"\"\n",
    "    Applies the rotary embeddings to queries or keys.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    qk: torch.Tensor of size (N, H, L, E / H)\n",
    "        The queries or keys intepreted as complex numbers (with contiguous real and imaginary parts).\n",
    "    angles: torch.Tensor of size (L, (E / H) / 2)\n",
    "        The angles to apply.\n",
    "    \"\"\"\n",
    "    qk_complex = torch.view_as_complex(qk.reshape(*qk.shape[:-1], -1, 2))\n",
    "    qk_rot = torch.view_as_real(qk_complex * angles).flatten(-2)\n",
    "    return qk_rot.type_as(qk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = TransformerConfig(\n",
    "    vocab_size=vocab_size,\n",
    "    emb_dim=emb_dim,\n",
    "    n_head=n_head,\n",
    "    ffn_dropout=dropout,\n",
    "    causal=causal,\n",
    "    attn_bias=False,\n",
    "    attn_dropout=0,\n",
    "    rope=True,\n",
    "    rope_theta=10_000,\n",
    "    seq_len=4,\n",
    ")\n",
    "\n",
    "module = CrossAttention(config)\n",
    "z = module(x, y)\n",
    "\n",
    "module = SelfAttention(config)\n",
    "z = module(x)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us test if our implementation works with other dtypes and device."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "module = CrossAttention(config)\n",
    "module = module.half()\n",
    "module = module.to('cuda')\n",
    "\n",
    "x_, y_ = x.half(), y.half()\n",
    "x_, y_ = x_.to('cuda'), y_.to('cuda')\n",
    "\n",
    "z = module(x_, y_)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feedforward block\n",
    "\n",
    "Attention layers are followed in transformer by a multi-layer perceptron with one hidden layer.\n",
    "Several activations could be used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SwiGLU(nn.Module):\n",
    "    \"\"\"\n",
    "    Swish-Gated Linear Unit (SwiGLU) activation function.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    fan_in: int\n",
    "        input dimension\n",
    "    \"\"\"\n",
    "    def __init__(self, fan_in):\n",
    "        super().__init__()\n",
    "        self.fc = nn.Linear(fan_in, fan_in, bias=False)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return F.silu(x) * self.fc(x)\n",
    "\n",
    "\n",
    "hidden_dim = 4 * emb_dim\n",
    "\n",
    "fc1 = nn.Linear(emb_dim, hidden_dim, bias=False)\n",
    "fc2 = nn.Linear(hidden_dim, emb_dim, bias=False)\n",
    "\n",
    "activation_name = \"swiglu\"\n",
    "match activation_name:\n",
    "    case \"relu\":\n",
    "        activation = F.relu\n",
    "    case \"gelu\":\n",
    "        activation = F.gelu\n",
    "    case \"swiglu\":\n",
    "        activation = SwiGLU(hidden_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "out = fc1(x)\n",
    "out = activation(out)\n",
    "out = F.dropout(out, p=dropout, training=training)\n",
    "out = fc2(out)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us write this in a class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llmtuto.model.transformer import FeedForward, TransformerConfig\n",
    "\n",
    "\n",
    "model = FeedForward(TransformerConfig(emb_dim=E, ffn_dim=8 * E))\n",
    "z = model(x)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transformer block\n",
    "\n",
    "We are now ready to build a transformer block. It consists of a self-attention layer followed by a feedforward layer, together with normalization and residual connections.\n",
    "\n",
    "There are two main variants, whether layer normalization is done before or after the attention and feedforward layers (i.e., after or before the residual connection).\n",
    "Thinking that residual connection helps by allowing the model to parameterize small changes to the input, it is better to put the normalization after the residual connection, which corresponds to the \"pre-norm\" implementation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = TransformerConfig(\n",
    "    emb_dim = E,\n",
    "\n",
    "    # Attention parameters\n",
    "    n_head = H,\n",
    "    causal = True,\n",
    "    attn_bias = False,\n",
    "    attn_dropout = 0.0,\n",
    "    rope = True,\n",
    "    seq_len = L,\n",
    "    rope_theta = 10_000,\n",
    "\n",
    "    # Feed-forward parameters\n",
    "    activation = \"swiglu\",\n",
    "    ffn_dim = None,\n",
    "    ffn_bias = False,\n",
    "    ffn_dropout = 0.0,\n",
    "\n",
    "    # Transformer block parameter\n",
    "    pre_norm = True,\n",
    ")\n",
    "\n",
    "\n",
    "# Pytorch 2.0.1 does not have LayerNorm without bias\n",
    "if torch.__version__ < '2.1':\n",
    "    class LayerNorm(nn.Module):\n",
    "        def __init__(self, fan_in, bias):\n",
    "            super().__init__()\n",
    "            self.weight = nn.Parameter(torch.ones(fan_in))\n",
    "            self.bias = nn.Parameter(torch.zeros(fan_in)) if bias else None\n",
    "\n",
    "        def forward(self, x):\n",
    "            return F.layer_norm(x, normalized_shape=self.weight.shape, weight=self.weight, bias=self.bias, eps=1e-5)\n",
    "else:\n",
    "    LayerNorm = nn.LayerNorm\n",
    "\n",
    "\n",
    "ln_1 = LayerNorm(config.emb_dim, bias=False)\n",
    "attn = SelfAttention(config)\n",
    "ln_2 = LayerNorm(config.emb_dim, bias=False)\n",
    "ffn = FeedForward(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "if config.pre_norm:\n",
    "    out = x + attn(ln_1(x))\n",
    "    out = out + ffn(ln_2(out))\n",
    "else:\n",
    "    out = x + ln_1(attn(x))\n",
    "    out = out + ln_2(ffn(out))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transformer architecture\n",
    "\n",
    "Let us wrap everything into a (GPT-like) decoder only architecture.\n",
    "We have not discuss position encoding.\n",
    "We can define it through embeddings to be learned, and add together position and toekn embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llmtuto.model.transformer import TransformerBlock\n",
    "\n",
    "config = TransformerConfig(\n",
    "    # Embedding parameters\n",
    "    vocab_size = 32_768,\n",
    "    emb_dim = 512,\n",
    "\n",
    "    # Attention parameters\n",
    "    n_head = 16,\n",
    "    causal = True,\n",
    "    attn_bias = False,\n",
    "    attn_dropout = 0.0,\n",
    "    rope = True,\n",
    "    seq_len = 1024,\n",
    "    rope_theta = 10_000,\n",
    "\n",
    "    # Feed-forward parameters\n",
    "    activation = \"swiglu\",\n",
    "    ffn_dim = None,\n",
    "    ffn_bias = False,\n",
    "    ffn_dropout = 0.0,\n",
    "\n",
    "    # Transformer block parameter\n",
    "    norm = 'layer',\n",
    "    norm_bias = False,\n",
    "    pre_norm = True,\n",
    "\n",
    "    # Transformer parameters\n",
    "    n_layer = 12,\n",
    "    emb_dropout = 0.0,\n",
    "    pos_emb = True,\n",
    ")\n",
    "\n",
    "\n",
    "token_emb = nn.Embedding(config.vocab_size, config.emb_dim)\n",
    "pos_emb = nn.Embedding(config.seq_len, config.emb_dim)\n",
    "\n",
    "transformer = nn.Sequential(\n",
    "    *(TransformerBlock(config) for _ in range(config.n_layer))\n",
    ")\n",
    "output = nn.Linear(config.emb_dim, config.vocab_size, bias=False)\n",
    "output.weight = token_emb.weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "L = config.seq_len\n",
    "N = 4\n",
    "x = torch.randint(0, config.vocab_size, (N, L))\n",
    "\n",
    "xte = token_emb(x)\n",
    "xpe = pos_emb(torch.arange(L))\n",
    "z = xte + xpe\n",
    "z = F.dropout(z, p=config.emb_dropout, training=True)\n",
    "z_out = transformer(z)\n",
    "out = output(z_out)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us try our implementation on GPU with float16."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda'\n",
    "x = x.to(device)\n",
    "token_emb = token_emb.half().to(device)\n",
    "pos_emb = pos_emb.half().to(device)\n",
    "transformer = transformer.half().to(device)\n",
    "output = output.half().to(device)\n",
    "\n",
    "xte = token_emb(x)\n",
    "xpe = pos_emb(torch.arange(L, device=device))\n",
    "z = F.dropout(xte + xpe, p=config.emb_dropout, training=True)\n",
    "z_out = transformer(z)\n",
    "out = output(z_out)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We end up by wrapping everything into a class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llmtuto.model.transformer import CausalTransformer\n",
    "\n",
    "\n",
    "model = CausalTransformer(config).to(device)\n",
    "out = model(x)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dev",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
