{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data: modular addition\n",
    "\n",
    "We will start with a problem that consists in adding or substracting two numbers modulo an other number."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def addition(x, y, z):\n",
    "    return (x + y) % z\n",
    "\n",
    "def substraction(x, y, z):\n",
    "    return (x - y) % z"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will consider two scenarios:\n",
    "- z is fixed to be 60.\n",
    "- z could be 2, 3, 4, 5 or 6.\n",
    "\n",
    "Note that because 60 divides all the other numbers, solving the second tasks could be done by solving the first tasks and learning the modulo operation `x % z`."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenization\n",
    "We are going to use language modeling to solve this problem.\n",
    "To do so, we need to define a vocabulary.\n",
    "We will work in base 60 and consider each number as a token.\n",
    "We will equally consider the token `+` and `-`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens_list = [k for k in range(60)] + ['+', '-']\n",
    "vocab_size = len(tokens_list)\n",
    "\n",
    "tokens_id = {tokens_list[k]: k for k in range(len(tokens_list))}"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset generation\n",
    "\n",
    "We will choose a simple data generation that simple consider all possible pairwise operations.\n",
    "\n",
    "For the case where `z=60` is fixed, we only consider addition for the moment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import os\n",
    "from llmtuto.config import DATA_DIR\n",
    "\n",
    "\n",
    "data = []\n",
    "for x in range(60):\n",
    "    for y in range(60):\n",
    "        out = addition(x, y, 60)\n",
    "        data.append([tokens_id[x], tokens_id[y], tokens_id[out]])\n",
    "\n",
    "        # out = addition(x, y, 60)\n",
    "        # data.append([tokens_id[x], tokens_id['+'], tokens_id[y], tokens_id[out]])\n",
    "\n",
    "        # out = substraction(x, y, 60)\n",
    "        # data.append([tokens_id[x], tokens_id['-'], tokens_id[y], tokens_id[out]])\n",
    "\n",
    "\n",
    "save_dir = DATA_DIR / 'single_base'\n",
    "os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "with open(save_dir / f'data.csv', 'w') as f:\n",
    "    writer = csv.writer(f)\n",
    "    writer.writerows(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = []\n",
    "zs = [2, 3, 4, 5, 6]\n",
    "\n",
    "for x in range(60):\n",
    "    for y in range(60):\n",
    "        for z in zs:\n",
    "            out = addition(x, y, z)\n",
    "            data.append([tokens_id[x], tokens_id['+'], tokens_id[y], tokens_id[z], tokens_id[out]])\n",
    "\n",
    "            out = substraction(x, y, z)\n",
    "            data.append([tokens_id[x], tokens_id['-'], tokens_id[y], tokens_id[z], tokens_id[out]])\n",
    "\n",
    "\n",
    "save_dir = DATA_DIR / 'multi_base'\n",
    "os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "with open(save_dir / 'data.csv', 'w') as f:\n",
    "    writer = csv.writer(f)\n",
    "    writer.writerows(data)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train/test split\n",
    "\n",
    "We keep 80 percent of the data for training, randomly permute them, and save the result as torch tensors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "def count_file_size(file_path: str) -> (int, int):\n",
    "    \"\"\" Count lines and columns of a csv file.\"\"\"\n",
    "    line_count = column_count = 0\n",
    "    with open(file_path) as f:\n",
    "        while f.readline():\n",
    "            if not column_count:\n",
    "                column_count = len(f.readline().split(','))\n",
    "            line_count += 1\n",
    "        line_count += 1\n",
    "    return line_count, column_count\n",
    "\n",
    "\n",
    "def get_datasplit(problem, train_percentage, rng, dtype=np.int32):\n",
    "    \"\"\" Split dataset into train and test set.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    problem: str\n",
    "        Name of the problem, either 'single_base' or 'multi_base'.\n",
    "    train_percentage: float\n",
    "        Percentage of data to be used for training.\n",
    "    rng: numpy.random.Generator\n",
    "        Random number generator.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    x_train: numpy.ndarray\n",
    "        Training input data.\n",
    "    y_train: numpy.ndarray\n",
    "        Training output data.\n",
    "    x_test: numpy.ndarray\n",
    "        Test input data.\n",
    "    y_test: numpy.ndarray\n",
    "        Test output data.\n",
    "    \"\"\"\n",
    "\n",
    "    save_dir = DATA_DIR / problem\n",
    "    try:\n",
    "        n_data, seq_dim = count_file_size(save_dir / 'data.csv')\n",
    "    except FileNotFoundError:\n",
    "        raise FileNotFoundError(f\"Raw data file {save_dir}/data.csv not found.\")\n",
    "\n",
    "    n_train_bool_idx = rng.random(n_data) < train_percentage\n",
    "    n_train = np.sum(n_train_bool_idx).astype(dtype)\n",
    "    n_test = n_data - n_train\n",
    "\n",
    "    train = np.empty((n_train, seq_dim), dtype=dtype)\n",
    "    test = np.empty((n_test, seq_dim), dtype=dtype)\n",
    "    with open(save_dir / 'data.csv') as f:\n",
    "        csv_reader = csv.reader(f)\n",
    "        train_idx = test_idx = 0\n",
    "        for i, row in enumerate(csv_reader):\n",
    "            if n_train_bool_idx[i]:\n",
    "                train[train_idx] = row\n",
    "                train_idx += 1\n",
    "            else:\n",
    "                test[test_idx] = row\n",
    "                test_idx += 1\n",
    "    \n",
    "    train = rng.permutation(train, axis=0)\n",
    "    x_train, y_train = train[:, :-1], train[:, -1]\n",
    "    x_test, y_test = test[:, :-1], test[:, -1]\n",
    "\n",
    "    return x_train, y_train, x_test, y_test\n",
    "\n",
    "\n",
    "#------------------------------#\n",
    "# Save data as pytorch tensors #\n",
    "#------------------------------#\n",
    "\n",
    "import torch\n",
    "\n",
    "train_percentage = 0.8\n",
    "rng = np.random.default_rng(12345)\n",
    "\n",
    "for problem in ['single_base', 'multi_base']:\n",
    "    x_train, y_train, x_test, y_test = get_datasplit(problem, train_percentage, rng)\n",
    "\n",
    "    x_train = torch.tensor(x_train)\n",
    "    y_train = torch.tensor(y_train)\n",
    "    x_test = torch.tensor(x_test)\n",
    "    y_test = torch.tensor(y_test)\n",
    "\n",
    "    torch.save(x_train, DATA_DIR / problem / f'x_train.pt')\n",
    "    torch.save(y_train, DATA_DIR / problem / f'y_train.pt')\n",
    "    torch.save(x_test, DATA_DIR / problem / f'x_test.pt')\n",
    "    torch.save(y_test, DATA_DIR / problem / f'y_test.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dev",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
