{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load pretrained models\n",
    "\n",
    "The previous notebook coded transformer architectures.\n",
    "Let us load pretrained model onto those architecture.\n",
    "Most open-source model can be loaded through the HuggingFace Transformers library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "\n",
    "from llmtuto.model.transformer import TransformerConfig, CausalTransformer\n",
    "\n",
    "model = \"gpt2\"  # Options are \"gpt2\", \"gpt2-medium\", \"gpt2-large\", \"gpt2-xl\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We start with GPT-2.\n",
    "Let us load the model specifications."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llmtuto.model.transformer import TransformerConfig\n",
    "\n",
    "\n",
    "config_args = {\n",
    "    'gpt2':   dict(emb_dim=768,  n_head=12, n_layer=12),  # 124M params\n",
    "    'gpt2-medium':  dict(emb_dim=1024, n_head=16, n_layer=24),  # 350M params\n",
    "    'gpt2-large':   dict(emb_dim=1280, n_head=20, n_layer=36),  # 774M params\n",
    "    'gpt2-xl':      dict(emb_dim=1600, n_head=25, n_layer=48),  # 1558M params\n",
    "}[model]\n",
    "config_args = config_args | dict(\n",
    "    vocab_size=50_257,\n",
    "    pos_emb=True,\n",
    "    seq_len=1024, \n",
    "    attn_bias=True,\n",
    "    ffn_bias=True,\n",
    "    norm_bias=True,\n",
    "    activation=\"gelu\",\n",
    "    norm=\"layer\",\n",
    "    pre_norm=True,\n",
    "    weight_tying=True,\n",
    ")\n",
    "config = TransformerConfig(**config_args)\n",
    "gpt2 = CausalTransformer(config)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The weight can be downloaded from HuggingFace Transformers library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPT2LMHeadModel(\n",
      "  (transformer): GPT2Model(\n",
      "    (wte): Embedding(50257, 768)\n",
      "    (wpe): Embedding(1024, 768)\n",
      "    (drop): Dropout(p=0.1, inplace=False)\n",
      "    (h): ModuleList(\n",
      "      (0-11): 12 x GPT2Block(\n",
      "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        (attn): GPT2Attention(\n",
      "          (c_attn): Conv1D()\n",
      "          (c_proj): Conv1D()\n",
      "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
      "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        (mlp): GPT2MLP(\n",
      "          (c_fc): Conv1D()\n",
      "          (c_proj): Conv1D()\n",
      "          (act): NewGELUActivation()\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "  )\n",
      "  (lm_head): Linear(in_features=768, out_features=50257, bias=False)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "from transformers import GPT2LMHeadModel\n",
    "\n",
    "model_hf = GPT2LMHeadModel.from_pretrained(model)\n",
    "print(model_hf)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some work is needed to cast the model into our own class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gpt_state_dict = model_hf.state_dict()\n",
    "local_state_dict = gpt2.state_dict()\n",
    "\n",
    "correspondence = {\n",
    "    \"embeddings.token_emb.weight\": \"transformer.wte.weight\",\n",
    "    \"embeddings.pos_emb.weight\": \"transformer.wpe.weight\",\n",
    "    \"output.weight\": \"lm_head.weight\",\n",
    "    \"output_norm.weight\":   \"transformer.ln_f.weight\",\n",
    "    \"output_norm.bias\":     \"transformer.ln_f.bias\",\n",
    "}\n",
    "transposed = []\n",
    "special = {}\n",
    "for layer in range(config.n_layer):\n",
    "    correspondence = correspondence | {\n",
    "        f\"blocks.{layer}.norm_1.weight\":       f\"transformer.h.{layer}.ln_1.weight\",\n",
    "        f\"blocks.{layer}.norm_1.bias\":         f\"transformer.h.{layer}.ln_1.bias\",\n",
    "        f\"blocks.{layer}.attn.output.weight\":  f\"transformer.h.{layer}.attn.c_proj.weight\",\n",
    "        f\"blocks.{layer}.attn.output.bias\":    f\"transformer.h.{layer}.attn.c_proj.bias\",\n",
    "        f\"blocks.{layer}.norm_2.weight\":       f\"transformer.h.{layer}.ln_2.weight\",\n",
    "        f\"blocks.{layer}.norm_2.bias\":         f\"transformer.h.{layer}.ln_2.bias\",\n",
    "        f\"blocks.{layer}.ffn.fc1.weight\":      f\"transformer.h.{layer}.mlp.c_fc.weight\",\n",
    "        f\"blocks.{layer}.ffn.fc1.bias\":        f\"transformer.h.{layer}.mlp.c_fc.bias\",\n",
    "        f\"blocks.{layer}.ffn.fc2.weight\":      f\"transformer.h.{layer}.mlp.c_proj.weight\",\n",
    "        f\"blocks.{layer}.ffn.fc2.bias\":        f\"transformer.h.{layer}.mlp.c_proj.bias\",\n",
    "    }\n",
    "    transposed = transposed + [\n",
    "        f\"transformer.h.{layer}.attn.c_attn.weight\",\n",
    "        f\"transformer.h.{layer}.attn.c_proj.weight\",\n",
    "        f\"transformer.h.{layer}.mlp.c_fc.weight\",\n",
    "        f\"transformer.h.{layer}.mlp.c_proj.weight\",\n",
    "    ]\n",
    "    special = special | {\n",
    "        f\"blocks.{layer}.attn.query.weight\": f\"transformer.h.{layer}.attn.c_attn.weight\",\n",
    "        f\"blocks.{layer}.attn.query.bias\":   f\"transformer.h.{layer}.attn.c_attn.bias\",\n",
    "        f\"blocks.{layer}.attn.key.weight\":   f\"transformer.h.{layer}.attn.c_attn.weight\",\n",
    "        f\"blocks.{layer}.attn.key.bias\":     f\"transformer.h.{layer}.attn.c_attn.bias\",\n",
    "        f\"blocks.{layer}.attn.value.weight\": f\"transformer.h.{layer}.attn.c_attn.weight\",\n",
    "        f\"blocks.{layer}.attn.value.bias\":   f\"transformer.h.{layer}.attn.c_attn.bias\",\n",
    "    }\n",
    "for k in correspondence:\n",
    "    if correspondence[k] in transposed:\n",
    "        local_state_dict[k] = gpt_state_dict[correspondence[k]].T\n",
    "    else:\n",
    "        local_state_dict[k] = gpt_state_dict[correspondence[k]]\n",
    "for k in special:\n",
    "    if 'query' in k:\n",
    "        if 'bias' in k:\n",
    "            local_state_dict[k] = gpt_state_dict[special[k]][:config.emb_dim]\n",
    "        else:\n",
    "            local_state_dict[k] = gpt_state_dict[special[k]][:, :config.emb_dim].T\n",
    "    elif 'key' in k:\n",
    "        if 'bias' in k:\n",
    "            local_state_dict[k] = gpt_state_dict[special[k]][config.emb_dim:2*config.emb_dim]\n",
    "        else:\n",
    "            local_state_dict[k] = gpt_state_dict[special[k]][:, config.emb_dim:2*config.emb_dim].T\n",
    "    elif 'value' in k:\n",
    "        if 'bias' in k:\n",
    "            local_state_dict[k] = gpt_state_dict[special[k]][2*config.emb_dim:]\n",
    "        else:\n",
    "            local_state_dict[k] = gpt_state_dict[special[k]][:, 2*config.emb_dim:].T\n",
    "gpt2.load_state_dict(local_state_dict)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us wrap this code in a function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:llmtuto.model.pretrained_transformer:Loading GPT2-small tokenizer from Tiktoken\n",
      "INFO:llmtuto.model.pretrained_transformer:Loading GPT2 model from /private/home/vivc/code/arc-agi/libraries/LLMTutorial/data/pretrained/gpt2-small.pt\n",
      "/private/home/vivc/code/arc-agi/libraries/LLMTutorial/src/llmtuto/model/pretrained_transformer.py:70: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  self.model.load_state_dict(torch.load(save_path))\n",
      "INFO:llmtuto.model.pretrained_transformer:Loading GPT2-small tokenizer from Tiktoken\n",
      "INFO:llmtuto.model.pretrained_transformer:Loading GPT2 model from /private/home/vivc/code/arc-agi/libraries/LLMTutorial/data/pretrained/gpt2-small.pt\n",
      "INFO:llmtuto.model.pretrained_transformer:Loading GPT2-medium tokenizer from Tiktoken\n",
      "INFO:llmtuto.model.pretrained_transformer:Loading GPT2 model from /private/home/vivc/code/arc-agi/libraries/LLMTutorial/data/pretrained/gpt2-medium.pt\n",
      "INFO:llmtuto.model.pretrained_transformer:Loading GPT2-large tokenizer from Tiktoken\n",
      "INFO:llmtuto.model.pretrained_transformer:Loading GPT2 model from /private/home/vivc/code/arc-agi/libraries/LLMTutorial/data/pretrained/gpt2-large.pt\n",
      "INFO:llmtuto.model.pretrained_transformer:Loading GPT2-xl tokenizer from Tiktoken\n",
      "INFO:llmtuto.model.pretrained_transformer:Loading GPT2 model from /private/home/vivc/code/arc-agi/libraries/LLMTutorial/data/pretrained/gpt2-xl.pt\n"
     ]
    }
   ],
   "source": [
    "import logging\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from llmtuto.config import DATA_DIR\n",
    "from llmtuto.model.pretrained_transformer import GPT2\n",
    "\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "\n",
    "gpt2 = GPT2(model='small', save_dir=DATA_DIR / 'pretrained')\n",
    "\n",
    "for model in ['small', 'medium', 'large', 'xl']:\n",
    "    gpt2 = GPT2(model=model, save_dir=DATA_DIR / 'pretrained')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sentence generation\n",
    "\n",
    "GPT models are useful to generate sentences, eventually conditioned on a prompt.\n",
    "To turn the numbers into words, we need to load the tokenizer.\n",
    "The tokenizer compatible with GPT-2 is available from OpenAI tiktoken library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model to generate from\n",
    "device = torch.device(\"cuda:1\" if torch.cuda.is_available() else \"cpu\")\n",
    "transformer_model = gpt2\n",
    "transformer_model.to(device)\n",
    "transformer_model.eval();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Machine Learning is  a branch of computer science that deals with the problem of learning from data.  It is a branch of computer science that deals with the problem of learning from data.  It is a branch of computer science that deals with the problem of learning from data.  It is a branch of computer science that deals with the problem of learning from data.  It is a branch of computer science that deals with the problem of learning from data.  It is a branch of computer science that deals with the problem<EOS>\n",
      "I am a big fan of large language models. I think they are a great way to model complex systems. I also think that they are a great way to model the human mind.\n",
      "\n",
      "I have been working on a large language model for a while now. It is a model of the human mind. It is a model of the human brain. It is a model of the human mind-computer interface. It is a model of the human mind-brain interface. It is a model of the human mind-brain interface. It is a<EOS>\n",
      "Women are specially good at  being able to see the big picture.  They are good at seeing the big picture of the world, and they are good at seeing the big picture of their own lives.  They are good at seeing the big picture of their own lives, and they are good at seeing the big picture of the world.  They are good at seeing the big picture of the world, and they are good at seeing the big picture of their own lives.  They are good at seeing the big picture of<EOS>\n"
     ]
    }
   ],
   "source": [
    "import tiktoken\n",
    "\n",
    "prompts = [\n",
    "    \"Machine Learning is \",\n",
    "    \"I am a big fan of large language models.\",\n",
    "    \"Women are specially good at \",\n",
    "]\n",
    "\n",
    "# tokenize sentence\n",
    "tokenizer = tiktoken.get_encoding('gpt2')\n",
    "\n",
    "if isinstance(prompts, str):\n",
    "    prompts = [prompts]\n",
    "tokens = [tokenizer.encode(seq) for seq in prompts]\n",
    "\n",
    "seq_len = 100\n",
    "random = False\n",
    "temperature = 1.\n",
    "\n",
    "if random:\n",
    "    def choice_token_from_logit(logits):\n",
    "        logits /= temperature\n",
    "        probs = F.softmax(logits, dim=-1)\n",
    "        return torch.multinomial(probs, num_samples=1)\n",
    "else:\n",
    "    def choice_token_from_logit(logits):\n",
    "        return torch.argmax(logits, dim=-1, keepdim=True)\n",
    "\n",
    "# handling different sentences lengths\n",
    "lenghts = [len(seq) for seq in tokens]\n",
    "nb_sentences, max_len, min_len = len(prompts), max(lenghts), min(lenghts)\n",
    "\n",
    "seq_idx = torch.zeros((nb_sentences, max_len), dtype=torch.long, device=device)\n",
    "mask = torch.zeros((nb_sentences, max_len - min_len), dtype=torch.bool, device=device)\n",
    "for i, seq in enumerate(tokens):\n",
    "    seq_idx[i, :len(seq)] = torch.tensor(seq, dtype=torch.long, device=device)\n",
    "    mask[i, :len(seq) - min_len] = 1\n",
    "\n",
    "for i in range(min_len, max_len):\n",
    "    logits = transformer_model(seq_idx[:, :i])[:, -1, :]\n",
    "    next_token = choice_token_from_logit(logits).squeeze()\n",
    "    torch.where(mask[:, i - min_len], seq_idx[:, i], next_token, out=seq_idx[:, i])\n",
    "\n",
    "# generation of new tokens\n",
    "with torch.no_grad():\n",
    "    for i in range(seq_len):\n",
    "        logits = transformer_model(seq_idx)[:, -1, :]\n",
    "        next_token = choice_token_from_logit(logits)\n",
    "        seq_idx = torch.cat([seq_idx, next_token], dim=-1)\n",
    "\n",
    "sentences = [tokenizer.decode(list(seq)) for seq in seq_idx]\n",
    "[print(sen, end='<EOS>\\n') for sen in sentences];"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the inner loop refills all tokens in the sentence in every loop, which does not utilize efficiently the regressive nature of causal transformer.\n",
    "We will see how to do better in a following notebook."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us wrap this in a function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hey there, tell me everything about large language models.\n",
      "\n",
      "(v0.1.0, last commit: May 05, 2012)\n",
      "\n",
      "Describes the subclass names, their string representations in the Int32 and their Scala.Vase representation. (Thanks Aron Bréchon for contributions.)\n",
      "\n",
      "(Note: We'll be gradually phasing out those older versions, but that's down the road.)\n",
      "\n",
      "Represents class array indices and subarrays, not the array objects themselves.\n",
      "\n",
      "Specifies a special band\n",
      "\n",
      "I am a big fan of large language models. They give you a machine-legality for the complex semantics that comes to mind during a human introduction. You can then write software that represents that, and departures from that are a lot easier than when your semantic is so fancy. I'm thinking about one that would have \"UB\" as an derivation of V and \"GAU\" as a transcription of GC. In software people like working with this kind of space, especially with UB as a future formal logic, it allows complex semantics to\n",
      "\n",
      "I am a big fan of large language models. They're very popular among the experienced -- some of the powers that be even use them for model training -- and I understand their strengths and weaknesses. In particular, some people forget that large languages are not strictly speaking searchable, and perhaps they're used as a lazy evaluation of the training data.\n",
      "\n",
      "Search, by the way, has a clever model. You pay for the frequency of the words of each sentence, and it starts valuing retention as the number of items in each epoch. As a\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from llmtuto.sample import language_generator\n",
    "\n",
    "prompts = [\n",
    "    \"Hey there, tell me everything about large language models.\",\n",
    "    \"I am a big fan of large language models.\",\n",
    "    \"I am a big fan of large language models.\",\n",
    "]\n",
    "\n",
    "transformer_model = transformer_model.to(device).eval()\n",
    "\n",
    "sentences = language_generator(prompts, tokenizer, transformer_model, seq_len=100, random=True, temperature=1., device=device)\n",
    "\n",
    "for sentence in sentences:\n",
    "    print(sentence)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "del transformer_model, sentence, logits, next_token, seq_idx, gpt2, tokens"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading Mistral 7B\n",
    "\n",
    "Let us redo everything from scratch with the Mistral 7B model.\n",
    "We will load the weight directly from Mistal website.\n",
    "Note that we could equally retake their code, which is available from their GitHub."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "import torch\n",
    "\n",
    "from llmtuto.model.transformer import TransformerConfig, CausalTransformer\n",
    "from llmtuto.config import DATA_DIR\n",
    "\n",
    "# run on bash or with subprocess\n",
    "# cd DATA_DIR / 'pretrained'\n",
    "# wget https://files.mistral-7b-v0-1.mistral.ai/mistral-7B-v0.1.tar\n",
    "# tar -xf mistral-7B-v0.1.tar\n",
    "\n",
    "model_path = DATA_DIR / 'mistral-7B-v0.1'\n",
    "model_path = Path('/private/home/vivc/models/mistral-7B-v0.1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "config_args = dict(\n",
    "    vocab_size= 32_000,\n",
    "    emb_dim=4096,\n",
    "    pos_emb=False,\n",
    "    seq_len=4096,\n",
    "    n_head=32,\n",
    "    attn_bias=False,\n",
    "    rope=True,\n",
    "    rope_theta=10_000,\n",
    "    activation=\"swiglu\",\n",
    "    ffn_dim=14336,\n",
    "    ffn_bias=False,\n",
    "    norm=\"rms\",\n",
    "    norm_bias=False,\n",
    "    pre_norm=True,\n",
    "    n_layer=32,\n",
    "    attn_downsampling=4,\n",
    "    norm_eps = 1e-3\n",
    ")\n",
    "\n",
    "config = TransformerConfig(**config_args)\n",
    "model = CausalTransformer(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_4148137/2052592844.py:22: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  mistral_state_dict = torch.load(model_path / 'consolidated.00.pth')\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "correspondence = {\n",
    "    \"embeddings.token_emb.weight\": \"tok_embeddings.weight\",\n",
    "    \"output_norm.weight\":          \"norm.weight\",\n",
    "    \"output.weight\":               \"output.weight\",\n",
    "}\n",
    "special = []\n",
    "special_keys = [\"attention.wq.weight\", \"attention.wk.weight\", \"attention.wv.weight\"]\n",
    "for layer in range(config.n_layer):\n",
    "    correspondence = correspondence | {\n",
    "        f\"blocks.{layer}.norm_1.weight\":         f\"layers.{layer}.attention_norm.weight\",\n",
    "        f\"blocks.{layer}.attn.query.weight\":     f\"layers.{layer}.attention.wq.weight\",\n",
    "        f\"blocks.{layer}.attn.key.weight\":       f\"layers.{layer}.attention.wk.weight\",\n",
    "        f\"blocks.{layer}.attn.value.weight\":     f\"layers.{layer}.attention.wv.weight\",\n",
    "        f\"blocks.{layer}.attn.output.weight\":    f\"layers.{layer}.attention.wo.weight\",\n",
    "        f\"blocks.{layer}.norm_2.weight\":         f\"layers.{layer}.ffn_norm.weight\",\n",
    "        f\"blocks.{layer}.ffn.fc1.weight\":        f\"layers.{layer}.feed_forward.w1.weight\",\n",
    "        f\"blocks.{layer}.ffn.fc2.weight\":        f\"layers.{layer}.feed_forward.w2.weight\",\n",
    "        f\"blocks.{layer}.ffn.swiglu_mat.weight\": f\"layers.{layer}.feed_forward.w3.weight\",\n",
    "    }\n",
    "\n",
    "local_state_dict = model.state_dict()\n",
    "mistral_state_dict = torch.load(model_path / 'consolidated.00.pth')\n",
    "for k in local_state_dict:\n",
    "    if k in correspondence:\n",
    "        local_state_dict[k] = mistral_state_dict[correspondence[k]]\n",
    "\n",
    "model.load_state_dict(local_state_dict)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us load the model tokenizer, which is based on `sentencepiece`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentencepiece import SentencePieceProcessor\n",
    "\n",
    "tokenizer = SentencePieceProcessor(model_file=str(model_path / 'tokenizer.model'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/private/home/vivc/miniconda/envs/llm/lib/python3.12/site-packages/torch/nn/modules/module.py:1160: UserWarning: Casting complex values to real discards the imaginary part (Triggered internally at ../aten/src/ATen/native/Copy.cpp:305.)\n",
      "  return t.to(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hey there, tell me everything about large language models.\n",
      "\n",
      "## What are large language models?\n",
      "\n",
      "Large language models are a type of machine learning model that is trained on a large amount of text data. These models are able to generate new text that is similar in style and content to the text\n"
     ]
    }
   ],
   "source": [
    "device = 'cuda:0'\n",
    "\n",
    "seq_idx = torch.tensor(tokenizer.encode('Hey there, tell me everything about large language models.'), device=device).unsqueeze(0)\n",
    "model = model.to(device, torch.float16)\n",
    "\n",
    "for i in range(50):\n",
    "    logits = model(seq_idx)[:, -1, :]\n",
    "    next_token = torch.argmax(logits, dim=-1, keepdim=True)\n",
    "    seq_idx = torch.cat([seq_idx, next_token], dim=-1)\n",
    "\n",
    "print(tokenizer.decode([a.item() for a in seq_idx[0]]), flush=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "del model, seq_idx, next_token, logits\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multi-GPU fast inference\n",
    "\n",
    "Even in half precision, Mistral is quite big and requires too much memory for long sentence generation on my single 16GB Quadro GPU.\n",
    "\n",
    "Yet, I have a second GPU, which is helpful to share the model weights on two GPUs, and be able to generate longer sentences."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Naive solution: Cut the model in half at the middle\n",
    "\n",
    "Let us load a model, and cast the first layers on GPU 0, and the last layers on GPU 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:mathllm.model.pretrained_transformer:Loading Mistral model from mistral checkpoint\n",
      "INFO:mathllm.model.pretrained_transformer:Loading Mistral tokenizer from mistral checkpoint\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CausalTransformer(\n",
      "  (embeddings): Embedding(\n",
      "    (token_emb): Embedding(32000, 4096)\n",
      "  )\n",
      "  (blocks): ModuleList(\n",
      "    (0-31): 32 x TransformerBlock(\n",
      "      (norm_1): RMSNorm()\n",
      "      (attn): SelfAttention(\n",
      "        (query): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "        (key): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "        (value): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "        (output): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "      )\n",
      "      (norm_2): RMSNorm()\n",
      "      (ffn): FeedForward(\n",
      "        (fc1): Linear(in_features=4096, out_features=14336, bias=False)\n",
      "        (fc2): Linear(in_features=14336, out_features=4096, bias=False)\n",
      "        (swiglu_mat): Linear(in_features=4096, out_features=14336, bias=False)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (output_norm): RMSNorm()\n",
      "  (output): Linear(in_features=4096, out_features=32000, bias=False)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "import logging\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from llmtuto.model.pretrained_transformer import GPT2, Mistral \n",
    "from llmtuto.config import DATA_DIR\n",
    "\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "\n",
    "# module = GPT2(model='small', save_dir=DATA_DIR / 'pretrained')\n",
    "module = Mistral(save_dir=DATA_DIR)\n",
    "model = module.model\n",
    "tokenizer = module.tokenizer\n",
    "print(model)\n",
    "vocab_size = model.embeddings.token_emb.weight.size(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Block1(nn.Module):\n",
    "    \"\"\"\n",
    "    Cast first half of a model on a device\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    model : nn.Module\n",
    "        neural network object\n",
    "    device : torch.device\n",
    "        device for computation\n",
    "\n",
    "    See Also\n",
    "    --------\n",
    "    CausalTranformer\n",
    "    \"\"\"\n",
    "    def __init__(self, model):\n",
    "        super().__init__()\n",
    "\n",
    "        # embeddings\n",
    "        self.embeddings = model.embeddings\n",
    "\n",
    "        # first blocks\n",
    "        n_layer = len(model.blocks)\n",
    "        self.blocks = nn.ModuleList([\n",
    "            model.blocks[i] for i in range(n_layer // 2)\n",
    "        ])\n",
    "        self.dropout = model.dropout\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = x\n",
    "        out = self.embeddings(out)\n",
    "        for block in self.blocks:\n",
    "            out = block(out)\n",
    "        return out\n",
    "\n",
    "class Block2(nn.Module):\n",
    "    \"\"\"\n",
    "    Cast second half of a model on a device\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    model : nn.Module\n",
    "        neural network object\n",
    "    device : torch.device\n",
    "        device for computation\n",
    "\n",
    "    See Also\n",
    "    --------\n",
    "    CausalTranformer\n",
    "    \"\"\"\n",
    "    def __init__(self, model):\n",
    "        super().__init__()\n",
    "\n",
    "        # last blocks\n",
    "        n_layer = len(model.blocks)\n",
    "        self.blocks = nn.ModuleList([\n",
    "            model.blocks[i] for i in range(n_layer // 2, n_layer)\n",
    "        ])\n",
    "\n",
    "        # output layer\n",
    "        self.output_norm = model.output_norm\n",
    "        self.output = model.output\n",
    "\n",
    "        self.dropout = model.dropout\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = x\n",
    "        for block in self.blocks:\n",
    "            out = block(out)\n",
    "        out = self.output_norm(out)\n",
    "        out = F.dropout(x, p=self.dropout, training=self.training)\n",
    "        out = self.output(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/private/home/vivc/miniconda/envs/llm/lib/python3.11/site-packages/torch/nn/modules/module.py:1158: UserWarning: Casting complex values to real discards the imaginary part (Triggered internally at ../aten/src/ATen/native/Copy.cpp:299.)\n",
      "  return t.to(device, dtype if t.is_floating_point() or t.is_complex() else None, non_blocking)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Block1(\n",
      "  (embeddings): Embedding(\n",
      "    (token_emb): Embedding(32000, 4096)\n",
      "  )\n",
      "  (blocks): ModuleList(\n",
      "    (0-15): 16 x TransformerBlock(\n",
      "      (norm_1): RMSNorm()\n",
      "      (attn): SelfAttention(\n",
      "        (query): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "        (key): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "        (value): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "        (output): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "      )\n",
      "      (norm_2): RMSNorm()\n",
      "      (ffn): FeedForward(\n",
      "        (fc1): Linear(in_features=4096, out_features=14336, bias=False)\n",
      "        (fc2): Linear(in_features=14336, out_features=4096, bias=False)\n",
      "        (swiglu_mat): Linear(in_features=4096, out_features=14336, bias=False)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      ")\n",
      "Block2(\n",
      "  (blocks): ModuleList(\n",
      "    (0-15): 16 x TransformerBlock(\n",
      "      (norm_1): RMSNorm()\n",
      "      (attn): SelfAttention(\n",
      "        (query): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "        (key): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "        (value): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "        (output): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "      )\n",
      "      (norm_2): RMSNorm()\n",
      "      (ffn): FeedForward(\n",
      "        (fc1): Linear(in_features=4096, out_features=14336, bias=False)\n",
      "        (fc2): Linear(in_features=14336, out_features=4096, bias=False)\n",
      "        (swiglu_mat): Linear(in_features=4096, out_features=14336, bias=False)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (output_norm): RMSNorm()\n",
      "  (output): Linear(in_features=4096, out_features=32000, bias=False)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "device1 = \"cuda:0\"\n",
    "device2 = \"cuda:1\"\n",
    "dtype = torch.float16\n",
    "model_1 = Block1(model).to(device=device1, dtype=dtype)\n",
    "model_2 = Block2(model).to(device=device2, dtype=dtype)\n",
    "print(model_1)\n",
    "print(model_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.randint(0, vocab_size, (2, 10)).to(device=device1)\n",
    "with torch.no_grad():\n",
    "    h = model_1(x)\n",
    "    y = model_2(h.to(device2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del model_1, model_2, x, h, y\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Decreasing GPU idleness with model weight copy\n",
    "\n",
    "In the previous setup, the second GPU is idle while the first one is doing the forward pass on the first part of the network.\n",
    "However, we could copy the first layer of the model (now saved on the first GPU) to this second GPU, perform of the first layer forward pass on some other prompt in parallel. Then discard the first layer, and copy the second layer to the second GPU, and so on.\n",
    "\n",
    "This quite generic idea is the backbone idea of [Microsoft DeepSpeed](https://github.com/microsoft/DeepSpeed) original idea called ZeRO, for [zero redundancy optimizer](https://arxiv.org/abs/1910.02054)."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us implement this for inference with the `asyncio` library Python.\n",
    "First, let us write a basic example to get use to the library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Function g is done\n",
      "Function f is done\n",
      "That was function g\n",
      "That was function f\n"
     ]
    }
   ],
   "source": [
    "import asyncio\n",
    "\n",
    "async def f():\n",
    "    # Simulate some asynchronous task\n",
    "    await asyncio.sleep(2)\n",
    "    print(\"Function f is done\")\n",
    "    return \"That was function f\"\n",
    "\n",
    "async def g():\n",
    "    # Simulate another asynchronous task\n",
    "    await asyncio.sleep(1)\n",
    "    print(\"Function g is done\")\n",
    "    return \"That was function g\"\n",
    "\n",
    "async def my_print(string, t):\n",
    "    await asyncio.sleep(t)\n",
    "    print(string)\n",
    "\n",
    "# Create tasks for functions f and g\n",
    "task_f = asyncio.create_task(f())\n",
    "task_g = asyncio.create_task(g())\n",
    "\n",
    "# Wait for both tasks to complete\n",
    "result_f = await task_f\n",
    "result_g = await task_g\n",
    "\n",
    "task_f = asyncio.create_task(my_print(result_f, 1))\n",
    "task_g = asyncio.create_task(my_print(result_g, .5))\n",
    "\n",
    "await task_f\n",
    "await task_g"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For simplicity, let us distribute weights to GPU from CPU (rather than from GPU, which might be beneficial in terms of communication speed in practice)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_1_cpu = Block1(model).to(device='cpu', dtype=dtype)\n",
    "model_2_cpu = Block2(model).to(device='cpu', dtype=dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:first forward on cuda:0\n",
      "INFO:__main__:first forward on cuda:1\n",
      "INFO:__main__:second forward on cuda:0\n",
      "INFO:__main__:second forward on cuda:1\n"
     ]
    }
   ],
   "source": [
    "import logging\n",
    "logger = logging.getLogger(__name__)\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s %(message)s')\n",
    "\n",
    "time_sleep = 0\n",
    "async def fsdp_inference(x, device):\n",
    "    await asyncio.sleep(time_sleep)\n",
    "    out = x.to(device)\n",
    "    logger.info(f\"first forward on {device}\")\n",
    "    with torch.no_grad():\n",
    "        model = model_1_cpu.to(device)\n",
    "        out = model(out)\n",
    "        del model\n",
    "        torch.cuda.empty_cache()\n",
    "    await asyncio.sleep(time_sleep)\n",
    "    logger.info(f\"second forward on {device}\")\n",
    "    with torch.no_grad():\n",
    "        model = model_2_cpu.to(device)\n",
    "        out = model(out)\n",
    "        del model\n",
    "        torch.cuda.empty_cache()\n",
    "    return out\n",
    "\n",
    "vocab_size = 32_000\n",
    "x1 = torch.randint(0, vocab_size, (2, 20)).to(device=device1)\n",
    "x2 = torch.randint(0, vocab_size, (2, 20)).to(device=device2)\n",
    "\n",
    "inf1 = asyncio.create_task(fsdp_inference(x1, device1))\n",
    "inf2 = asyncio.create_task(fsdp_inference(x2, device2))\n",
    "\n",
    "out1, out2 = await asyncio.gather(inf1, inf2)\n",
    "\n",
    "del fsdp_inference\n",
    "del inf1, inf2\n",
    "del out1, out2\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Native tools\n",
    "\n",
    "The simple ideas presented above for the forward pass are the basis of multi-GPU processing.\n",
    "However, coding it for foward, backward and optimization state is a serious engineering project, which hopefully, is been tackled by several open-source solution.\n",
    "\n",
    "Native PyTorch tools implement these ideas in efficient way, and generalize them for the backward and the optimizer steps.\n",
    "Those tools are part of the `torch.distributed` module.\n",
    "At the time of writing, it has two main mode of operation: `ddp` (for distributed data parallel, which consists in processing data in parallel on different machines which all have a full copy of the model) and `fsdp` (fully sharded data parallel, which assimilates to what we have just seen).\n",
    "\n",
    "- TODO: implement FSDP for mistral 7B inference (restart the notebook there)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Faster generation with key-value caching\n",
    "\n",
    "The usefulness of causal transformer is their auto-regressive properties, which allow to generate sequences one token at a time.\n",
    "\n",
    "However, we are currently not leveraging this auto-regressive property at inference time.\n",
    "In particular, when a new token has such been generated, the generation of the next one only requires to compute the attention of the new token with the previous ones: there is no need to recompute the attention key and value for the previous tokens.\n",
    "\n",
    "- TODO: do it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dev",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
