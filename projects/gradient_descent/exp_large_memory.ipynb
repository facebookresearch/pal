{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EOS for population dynamics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import subprocess\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from scipy.linalg import eigh\n",
    "from matplotlib import rc\n",
    "\n",
    "plt.rc('font', family='serif', size=8)\n",
    "\n",
    "sys.path.append('.')\n",
    "from model import AssociativeMemory, get_embeddings\n",
    "\n",
    "torch.manual_seed(42)\n",
    "\n",
    "WIDTH = 8.5              # inches (from ICML style file)\n",
    "HEIGHT = 8.5 / 1.5     # golden ratio\n",
    "\n",
    "rc('font', family='serif', size=8)\n",
    "usetex = not subprocess.run(['which', 'pdflatex']).returncode\n",
    "rc('text', usetex=usetex)\n",
    "if usetex:\n",
    "    rc('text.latex', preamble=r'\\usepackage{times}')\n",
    "\n",
    "def f(x, epsilon=0):\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data(n, d):\n",
    "    \n",
    "    E = get_embeddings(n, d, norm=True)\n",
    "    U = get_embeddings(n, d, norm=True)\n",
    "    \n",
    "    alpha = 1.\n",
    "    proba = (torch.arange(n) + 1.) ** (-alpha)\n",
    "    proba /= proba.sum()\n",
    "    return E, U, proba\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# niter = 100\n",
    "# lrs = [5, 10, 20]\n",
    "# k = 1\n",
    "\n",
    "def get_res(E, U, proba, lrs, niter=100):\n",
    "    n, d = E.shape\n",
    "    all_x = torch.arange(n)\n",
    "    all_y = f(all_x)\n",
    "    res = {}\n",
    "    for lr in lrs:\n",
    "        # model\n",
    "        # model = AssociativeMemory(E, U, random_init=True, layer_norm=True)\n",
    "        model = AssociativeMemory(E, U, random_init=False)\n",
    "        model.to(torch.float64)\n",
    "        all_x.to(torch.float64)\n",
    "        all_y.to(torch.float64)\n",
    "\n",
    "        r = 2 # check progress on Wstar = sum_i<r ui ei'\n",
    "\n",
    "        Wstar = (model.UT[:,torch.arange(r)] @ model.E[torch.arange(r)]).T\n",
    "\n",
    "        star_scores = model.E[all_x[:10]] @ Wstar @ model.UT\n",
    "        star_acc = (proba[:10] * (star_scores.argmax(-1) == all_y[:10]).float()).sum()\n",
    "\n",
    "        optimizer = torch.optim.SGD(model.parameters(), lr=lr, momentum=0)\n",
    "        # scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=niter, eta_min=0)\n",
    "\n",
    "        losses = torch.zeros(niter)\n",
    "        accs = torch.zeros(niter)\n",
    "        accs_start = torch.zeros(niter)\n",
    "        accs_end = torch.zeros(niter)\n",
    "        all_accs = torch.zeros((niter, n))\n",
    "        eigenvals_pop = np.zeros(niter)\n",
    "        progress = torch.zeros(niter)\n",
    "        progress_orth = torch.zeros(niter)\n",
    "        all_scores = torch.zeros((niter, n))\n",
    "        all_margins = torch.zeros((niter, n))\n",
    "\n",
    "\n",
    "        for i in range(niter):\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # compute loss\n",
    "\n",
    "            score = model(all_x)\n",
    "            loss = (proba * F.cross_entropy(score, all_y, reduction='none')).sum()\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        #     scheduler.step()\n",
    "\n",
    "            # record statistics\n",
    "            losses[i] = loss.item()\n",
    "            accs[i] = (proba * (score.argmax(-1) == all_y).float()).sum()\n",
    "            accs_start[i] = ((score.argmax(-1) == all_y)[:d]).float().mean()\n",
    "            accs_end[i] = ((score.argmax(-1) == all_y)[d:2*d]).float().mean()\n",
    "            all_accs[i] = ((score.argmax(-1) == all_y)).float()\n",
    "            all_scores[i] = score.detach().diag()\n",
    "            all_margins[i] = (score.detach() - (score.detach() - torch.diag(torch.inf * torch.ones(n))).max(-1)[0]).diag()\n",
    "            population_hessian = model.hessian(all_x, proba)\n",
    "            eigenvals_pop[i] = lr * eigh(population_hessian.numpy(), eigvals_only=True, subset_by_index=[d*d-1, d*d-1]) / 2.\n",
    "        #     progress[i] = (torch.sum(Wstar * model.W.data).item()\n",
    "        #                        / torch.sqrt(torch.sum(model.W.data ** 2))\n",
    "        #                        / torch.sqrt(torch.sum(Wstar.data ** 2)))\n",
    "            progress[i] = (model.E[0] - model.E[1]) @ model.W.data @ (model.UT[:,0] - model.UT[:,1])\n",
    "            progress_orth[i] = (model.E[0] + model.E[1]) @ model.W.data @ (model.UT[:,0] - model.UT[:,1])\n",
    "\n",
    "        #     lrs[i] = scheduler.get_last_lr()[0]\n",
    "\n",
    "        res[lr] = {'losses': losses, 'accs': accs, 'accs_start': accs_start, 'all_margins': all_margins,\n",
    "                   'progress': progress, 'progress_orth': progress_orth}\n",
    "    return res"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "N = 5\n",
    "lrs = [3, 10, 20]\n",
    "\n",
    "for d in [3, 5, 10]:\n",
    "    torch.manual_seed(43)\n",
    "    E, U, proba = get_data(N, d)\n",
    "    res = get_res(E, U, proba, lrs, niter=100)\n",
    "\n",
    "    fig, ax = plt.subplots(1, 1, figsize=(.2 * WIDTH, .2 * HEIGHT))\n",
    "    for lr in res:\n",
    "        ax.plot(res[lr]['losses'][:30], label=f'$\\\\eta = {lr}$')\n",
    "    ax.set_xlabel(r'iteration $t$', fontsize=6)\n",
    "    ax.legend()\n",
    "    ax.set_ylabel(r'${\\cal L}(W_t)$')\n",
    "    ax.set_title(fr\"$d = {d}$\")\n",
    "    fig.savefig(f'figures/loss_N{N}_d{d}.pdf', pad_inches=0, bbox_inches='tight')\n",
    "\n",
    "    \n",
    "    for lr in res:\n",
    "        fig, ax = plt.subplots(1, 1, figsize=(.18 * WIDTH, .18 * HEIGHT))\n",
    "        leg = []\n",
    "        for i in range(5):\n",
    "            a, = ax.plot(res[lr]['all_margins'][:30,i])\n",
    "            leg.append(a)\n",
    "        ax.set_yticks([0,2])\n",
    "        ax.set_ylabel(r'$m_t(x)$')\n",
    "        ax.set_title(rf\"$d = {d},\\ \\eta = {lr}$\")\n",
    "        if lr == 20 and d == 10:\n",
    "            ax.legend(leg, [r'1', r'2', r'3', r'4', r'5'],fontsize=6, handlelength=1, ncol=2, frameon=True, loc='lower right')\n",
    "        if d != 10:\n",
    "            ax.set_xticks([])\n",
    "        else:\n",
    "            ax.set_xlabel(r'iteration $t$')\n",
    "        ax.grid(axis='y', alpha=.5)\n",
    "        fig.savefig(f'figures/margins_N{N}_d{d}_lr{lr}.pdf', pad_inches=0, bbox_inches='tight')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dev",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
