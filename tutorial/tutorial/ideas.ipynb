{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Follows-up ideas to notebook `1_data`\n",
    "- Look at what happens when `zs = [2, 3, 4, 5, 6, 7, 8, 9, 10]`: does the transformers learns circuits that operates independently for each basis, or does it learns modular addition in higher basis before doing modulo operations? i.e., 2 and 4 and 8 are solved by learning `x+y%8` and learning `x%z` for `z=2,4,8`.\n",
    "- Look at number that needs to be represented with two digits in our basis (does the transformer learn some base addition table and remainders in this case?)\n",
    "- Study the added benefits of positional encodings. \n",
    "- Study the difference between encoder only and encoder-decoder architectures (can we do encoder only yet with prediction of positional encodings for the output?).\n",
    "- Look at longer sentences `x1 + x2 - x3 % z`.\n",
    "- See if one can mimick chain-of-thought behaviors with those math tasks, i.e., the transformer works better if we ask to explicit reasoning step. To do so, we could have some special tokens. 'e1', 'e2', ..., which if presents asks the transoformer to only simplify one operation instead of giving the full answer (i.e., `e1, x1 + x2 + x3+ x4 -> x12 + x3 + x4` and `e2, x1+ x2 + x3 + x4 = e1, e1, x1 + x2 + x3 + x4 = x123 + x4`)\n",
    "- TODO Parse existing dataset such as `TinyStory`, `Shakespear`, `OpenOrca`, `Clip-like dataset`"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Follows-up to notebook `2_models`:\n",
    "- Implement encoder architecture as well as encoder-decoder architectures"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Follows-up ideas to notebook `3_pretrained_models`:\n",
    " - Can train a classifier that differentiate between languages based on some activation layers.\n",
    " - Use cache in attention to be able to generate tokens online without reprocessing past context (to be done in the attention module)."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
