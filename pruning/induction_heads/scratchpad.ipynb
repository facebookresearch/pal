{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A. Train the transformer model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7f1da7b32d70>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import pickle\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "sys.path.append(str(Path(\"..\").resolve()))\n",
    "save_dir = Path(\".\").resolve() / \"results\"\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from data import Dataset, DataArgs, generate_data\n",
    "from evals import attn1_score, attn2_score\n",
    "from models.transformer import Transformer, TransformerConfig\n",
    "\n",
    "DEVICE = torch.device(\"cpu\")  # torch.device('cuda:0')\n",
    "SEED = 42\n",
    "RNG = np.random.default_rng(SEED)\n",
    "np.random.seed(seed=SEED)\n",
    "torch.manual_seed(seed=SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Synthetic data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data\n",
    "vocab_size = 10\n",
    "bsz = 1024\n",
    "length = 128\n",
    "data = generate_data(vocab_size, bsz, length + 1)\n",
    "\n",
    "# Model\n",
    "config = TransformerConfig(\n",
    "    vocab_size=vocab_size,\n",
    "    emb_dim=32,\n",
    "    seq_len=length,\n",
    "    n_head=1,\n",
    "    n_layer=2,\n",
    ")\n",
    "model = Transformer(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adam\n",
      "TransformerConfig(vocab_size=10, emb_dim=32, pos_emb=True, pos_dim=32, freeze_pos=False, seq_len=128, emb_dropout=0.0, n_head=1, attn_bias=False, attn_dropout=0.0, rope=False, rope_theta=10000, activation='gelu', ffn_dim=128, ffn_bias=False, ffn_dropout=0.0, norm_layer=False, norm_eps=1e-05, pre_norm=True, n_layer=2, flash=True, weight_tying=False, output_dropout=0.0, dropout=0.0)\n"
     ]
    }
   ],
   "source": [
    "# Training\n",
    "niter = 1  # 100\n",
    "all_losses = {}\n",
    "all_accs = {}\n",
    "all_attn1 = {}\n",
    "all_attn2 = {}\n",
    "names = [\"Adam\"]  # ['SGD', 'Adam', 'AdamLN', 'SGDLN']\n",
    "\n",
    "X = data[:, :-1].to(dtype=torch.long, device=DEVICE)\n",
    "Y = data[:, 1:].to(dtype=torch.long, device=DEVICE)\n",
    "\n",
    "for name in names:\n",
    "    print(name, flush=True)\n",
    "\n",
    "    if name[-2:] == \"LN\":\n",
    "        config.norm_layer = True\n",
    "    else:\n",
    "        config.norm_layer = False\n",
    "    print(config, flush=True)\n",
    "\n",
    "    model = Transformer(config)\n",
    "    model.to(device=DEVICE)\n",
    "    if name[:3] == \"SGD\":\n",
    "        lr = 1e-2\n",
    "        optimizer = torch.optim.SGD(model.parameters(), lr=lr, momentum=0)\n",
    "    else:\n",
    "        lr = 1e-3\n",
    "        optimizer = torch.optim.Adam(model.parameters(), lr=lr, betas=(0, 0))\n",
    "\n",
    "    losses = torch.zeros(niter)\n",
    "    accs = torch.zeros(niter)\n",
    "    attn1 = torch.zeros(niter)\n",
    "    attn2 = torch.zeros(niter)\n",
    "\n",
    "    for i in range(niter):\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # compute loss\n",
    "        score, attns = model(X, verbose=True)\n",
    "        loss = F.cross_entropy(score.reshape((-1, vocab_size)), Y.reshape(-1))\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # record statistics\n",
    "        with torch.no_grad():\n",
    "            losses[i] = loss.item()\n",
    "            accs[i] = (score.argmax(-1) == Y).float().mean()\n",
    "            attn1[i] = attn1_score(attns[0, :, 0])\n",
    "            attn2[i] = attn2_score(attns[1, :, 0], X)\n",
    "\n",
    "    all_losses[name] = losses\n",
    "    all_accs[name] = accs\n",
    "    all_attn1[name] = attn1\n",
    "    all_attn2[name] = attn2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = os.path.join(save_dir, \"toy_data\")\n",
    "path = Path(path)\n",
    "path.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Save model\n",
    "torch.save(model.state_dict(), path / f\"{name}.pt\")\n",
    "\n",
    "# Save metrics\n",
    "with open(path / \"losses.pkl\", \"wb\") as f:\n",
    "    pickle.dump(all_losses, f)\n",
    "with open(path / \"accs.pkl\", \"wb\") as f:\n",
    "    pickle.dump(all_accs, f)\n",
    "with open(path / \"attn1.pkl\", \"wb\") as f:\n",
    "    pickle.dump(all_attn1, f)\n",
    "with open(path / \"attn2.pkl\", \"wb\") as f:\n",
    "    pickle.dump(all_attn2, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Shakespeare Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data\n",
    "bsz = 1024\n",
    "length = 128\n",
    "args = DataArgs(seq_length=length)\n",
    "ds = Dataset(args=args, train_test=None, bigram_outs=False)\n",
    "vocab_size = ds.num_tokens\n",
    "x, outs = ds.gen_batch(rng=RNG, batch_size=bsz)\n",
    "data = torch.from_numpy(x)\n",
    "\n",
    "# Model\n",
    "config = TransformerConfig(\n",
    "    vocab_size=vocab_size,\n",
    "    emb_dim=32,\n",
    "    seq_len=length,\n",
    "    n_head=1,\n",
    "    n_layer=2,\n",
    ")\n",
    "model = Transformer(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adam\n",
      "TransformerConfig(vocab_size=65, emb_dim=32, pos_emb=True, pos_dim=32, freeze_pos=False, seq_len=128, emb_dropout=0.0, n_head=1, attn_bias=False, attn_dropout=0.0, rope=False, rope_theta=10000, activation='gelu', ffn_dim=128, ffn_bias=False, ffn_dropout=0.0, norm_layer=False, norm_eps=1e-05, pre_norm=True, n_layer=2, flash=True, weight_tying=False, output_dropout=0.0, dropout=0.0)\n"
     ]
    }
   ],
   "source": [
    "# Training\n",
    "niter = 1  # 100\n",
    "all_losses = {}\n",
    "all_accs = {}\n",
    "all_attn1 = {}\n",
    "all_attn2 = {}\n",
    "names = [\"Adam\"]  # ['SGD', 'Adam', 'AdamLN', 'SGDLN']\n",
    "\n",
    "X = data[:, :-1].to(dtype=torch.long, device=DEVICE)\n",
    "Y = data[:, 1:].to(dtype=torch.long, device=DEVICE)\n",
    "\n",
    "for name in names:\n",
    "    print(name, flush=True)\n",
    "\n",
    "    if name[-2:] == \"LN\":\n",
    "        config.norm_layer = True\n",
    "    else:\n",
    "        config.norm_layer = False\n",
    "    print(config, flush=True)\n",
    "\n",
    "    model = Transformer(config)\n",
    "    model.to(device=DEVICE)\n",
    "    if name[:3] == \"SGD\":\n",
    "        lr = 1e-2\n",
    "        optimizer = torch.optim.SGD(model.parameters(), lr=lr, momentum=0)\n",
    "    else:\n",
    "        lr = 1e-3\n",
    "        optimizer = torch.optim.Adam(model.parameters(), lr=lr, betas=(0, 0))\n",
    "\n",
    "    losses = torch.zeros(niter)\n",
    "    accs = torch.zeros(niter)\n",
    "    attn1 = torch.zeros(niter)\n",
    "    attn2 = torch.zeros(niter)\n",
    "\n",
    "    for i in range(niter):\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # compute loss\n",
    "        score, attns = model(X, verbose=True)\n",
    "        loss = F.cross_entropy(score.reshape((-1, vocab_size)), Y.reshape(-1))\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # record statistics\n",
    "        with torch.no_grad():\n",
    "            losses[i] = loss.item()\n",
    "            accs[i] = (score.argmax(-1) == Y).float().mean()\n",
    "            attn1[i] = attn1_score(attns[0, :, 0])\n",
    "            attn2[i] = attn2_score(attns[1, :, 0], X)\n",
    "\n",
    "    all_losses[name] = losses\n",
    "    all_accs[name] = accs\n",
    "    all_attn1[name] = attn1\n",
    "    all_attn2[name] = attn2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = os.path.join(save_dir, \"shakespeare_data\")\n",
    "path = Path(path)\n",
    "path.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Save model\n",
    "torch.save(model.state_dict(), path / f\"{name}.pt\")\n",
    "\n",
    "# Save metrics\n",
    "with open(path / \"losses.pkl\", \"wb\") as f:\n",
    "    pickle.dump(all_losses, f)\n",
    "with open(path / \"accs.pkl\", \"wb\") as f:\n",
    "    pickle.dump(all_accs, f)\n",
    "with open(path / \"attn1.pkl\", \"wb\") as f:\n",
    "    pickle.dump(all_attn1, f)\n",
    "with open(path / \"attn2.pkl\", \"wb\") as f:\n",
    "    pickle.dump(all_attn2, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dev",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
